#problemas q ultilizam dados sequenciais
 
#usam loops ou repiti√ß√µes que permitem que a informa√ß√£o persista

#tipo se vc so usa fun√ß√£o d ativa√ß√£o e passa o valor pra frente vc n ta armazenando nada

#a descida do gradiente n funciona em redes neurais recorrentes pois ele vai ficando cada vez menor
#at√© chegar ao ponto q ele quase n altera o valor dos peso
#n√£o consegue armazenar dados muito distantes no tempo

#ai pra resolver isso existe Long Short Term Memory(LSTM)
#que ao ivez d s√≥ passar a informa√ß√£o pra frente faz (matematica) nos dados
#ai ele usa tanh + (matematica) e da certo üòÅüòÅüòÅ


#explica√ß√£o chata d cm funciona a matematica POREM importante
"""
1 decidir oq ser√° apagado =
sgmoid((saida_do_tempo_anterior-1) entrada_atual)
se o valor for 0 ele n √© improtante e ser√° apagado

2 decidir oq √© importante =
tanh(dados_q_s√£o_importantes)

3 decidir qual ser√° a saida =
usar sgmoid e tanh pra "filtrar" os dados, que ser√£o a saida
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
from pydantic import NoneStr
from sklearn.preprocessing import MinMaxScaler
from keras.callbacks import TensorBoard

treinamento = pd.read_csv('dataDL\petr4_treinamento.csv')
teste = pd.read_csv('dataDL\petr4_teste.csv')


plt.plot(treinamento['Open'])
plt.show()

treinamento = treinamento.dropna()
teste = teste.dropna()

# grafico = treinamento['Open'].hist(bins=50, figsize=(20,10))
# plt.show()
  
treinamento = treinamento.iloc[:, 1:2].values
teste = teste.iloc[:, 1:2].values
 

scaler = MinMaxScaler(feature_range=(0,1))
treinamento = scaler.fit_transform(treinamento)
teste = scaler.transform(teste)
 
#lembra q elas servem pra prever serias temporais? 
#ent√£o... pra ela saber q o pre√ßo √© do dia 17 √© 100 ela precisa do pre√ßo dos outros dias
#ai fica asssim = previsores = dias anteriores, classe = pre√ßo do dia q vc qr prever 

previsores = []
classe = []
for c in range(90, treinamento.shape[0]):
    previsores.append(treinamento[c-90:c, 0])
    #se c=90 ele vai do registro 0 ao 90, (0 √© a posi√ß√£o da coluna)
    classe.append(treinamento[c, 0])


previsores, classe = np.array(previsores), np.array(classe)
#os dados precis√£o ter 3d (previsores, tempo, input_dim)
previsores = np.reshape(previsores, (previsores.shape[0], previsores.shape[1], 1))
#previsores √© oq eu quero transformar, (previsores.shape[0] √© o numero de registros)
#previsores.shape[1] √© o numero de dias, 1 sgnifica que s√≥ tem uma coluna

rede_neural_recorrente = Sequential()
rede_neural_recorrente.add(LSTM(units=100, return_sequences=True, input_shape=(previsores.shape[1], 1)))
#units √© o numero d celulas d memoria(neuronios)
#return_sequences=truen qnd vc vai ter mais de uma camada d LSTM
#input_shape=(previsores.shape[1], 1)
#             numero d dias, 1 sgnifica que s√≥ tem uma coluna
rede_neural_recorrente.add(Dropout(0.3))

rede_neural_recorrente.add(LSTM(units=100, return_sequences=True))
rede_neural_recorrente.add(Dropout(0.3))
rede_neural_recorrente.add(LSTM(units=100))
#na ultima camada de LSTM o valor d Return_sequences √© false
rede_neural_recorrente.add(Dropout(0.3))

rede_neural_recorrente.add(Dense(units=1, activation='linear'))

rede_neural_recorrente.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_squared_error'])
#geralmente usa essas configura√ß√µes

tensorboard = TensorBoard(log_dir='logs/redes_neurais_recorrentes')

rede_neural_recorrente.fit(previsores, classe, epochs=100, batch_size=32, callbacks=[tensorboard])
#geralmente roda PELO MENOS 100 epocas
